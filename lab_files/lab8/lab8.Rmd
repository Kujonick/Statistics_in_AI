---
title: "Metody boostingowe"
output: html_document 
editor_options: 
  markdown: 
    wrap: 72
---

## Wprowadzenie

W ramach laboratorium zapoznamy się z boostingiem, czyli jedną z
najważniejszych technik zespołowego uczenia maszynowego (ensemble
learning). Przykładem poznanego na poprzednich laboratoriach uczenia
zespołowego są lasy losowe, gdzie drzewa decyzyjne/regresyjne były
dopasowywane do bootstrapowanych zbiorów uczących a na końcu ich wynik
uśredniany.

Boosting polega natomiast na sekwencyjnym trenowaniu modeli, gdzie każdy
kolejny próbuje skorygować błędy popełnione przez poprzednie (większą
wagę przypisuje się tym obserwacjom, które zostały błędnie
sklasyfikowane przez wcześniejsze modele). Fundament teoretyczny dla tej
metody treningu został zaprezentowany w pracy "Greedy Function
Approximation: A Gradient Boosting Machine"
(<https://jerryfriedman.su.domains/ftp/trebst.pdf>).

Laboratorium opiera się na dwóch z najpopularniejsych implementacjach
modeli boostingowych: XGBoost oraz CatBoost. Warto wspomnieć też o
LightGBM. Wszystkie trzy dostępne są zarówno w Pythonie oraz R. Strona z
dokumentacją dla każdego z narzędzi:

-   <https://xgboost.readthedocs.io/en/release_3.0.0/>
-   <https://catboost.ai>
-   <https://lightgbm.readthedocs.io/en/stable/>

Przed wykonaniem poniższych komórek należy zaktualizować środowisko
wykonawcze. Dla R: renv::restore() a dla pythona: pip install -r
requirements.txt

```{r setup}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(reticulate)
install.packages('xgboost', repos = c('https://dmlc.r-universe.dev', 'https://cloud.r-project.org'))
library(ISLR)
library(MASS)
library(tree)

library(xgboost)
```

```{python setup}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from catboost import CatBoostClassifier, CatBoostRegressor, Pool
```

## Przykład zastosowania

Wykorzystamy zbiór danych tooth growth. Zawiera dane dotyczące długości
odontoblastów (komórek odpowiedzialnych za wzrost zębów) u 10 świnek
morskich przy trzech poziomach dawkowania witaminy C (0,5, 1 i 2 mg) i
dwóch metodach podania (sok pomarańczowy lub kwas askorbinowy).
<https://rpubs.com/garedwards/107023>

```{r xgboost}
data(ToothGrowth)

y <- ToothGrowth$supp # Przewidujemy typ suplementu
x <- ToothGrowth[, c("len", "dose")] # Wybieramy predyktory - długość zęba i dawka 
model <- xgboost(x, y, nthreads = 1, nrounds = 2)
model
```

Prawdopodobieństwo dla ostatniej wartości zmiennej predykowanej (kwas
askorbinowy)

```{r xgboost-pred-prob}
predict(model, x[1:6, ], type = "response")
```

Logit

```{r xgboost-pred-prob}
predict(model, x[1:6, ], type = "raw")
```

Klasa o najwyższym prawdopodobieństwie

```{r xgboost-pred-class}
predict(model, x[1:6, ], type = "class")
```

XGBoost jest najbardziej elastyczną pod względem parametryzacji z
wymienionych bibliotek. Poniżej tylko mały podzbiór tego, jak możemy
wpływać na trening modelu. Wśród parametrów możemy na przykład znaleźć
parametr lambda redgularyzacji L2, która jest od razu zaimplementowana w
XGBoost, w przeciwieństwie do pozostałych biliotek

```{r xgboost-params}
y <- as.factor(ToothGrowth$supp)
x <- as.matrix(ToothGrowth[, c("len", "dose")])


model_conservative <- xgboost(
    x, y, nthreads = 1,
    nrounds = 5, # Rundy to w tym wypadku ilość modeli składowych
    max_depth = 2, # Maksymalna głębokość pojedynczego modelu
    reg_lambda = 0.5, # Regularyzacja L2 wag w liściach
    learning_rate = 0.15 # W jaki sposób zmniejsza się wartość kroku w algorytmie GB
)
pred_conservative <- predict(
    model_conservative,
    x
)
pred_conservative[1:6]
y[1:6]
```
```{r}
model_conservative <- xgboost(
    x, y, nthreads = 1,
    nrounds = 5, # Rundy to w tym wypadku ilość modeli składowych
    max_depth = 3, # Maksymalna głębokość pojedynczego modelu
    reg_lambda = 0.7, # Regularyzacja L2 wag w liściach
    learning_rate = 0.1 # W jaki sposób zmniejsza się wartość kroku w algorytmie GB
)
pred_conservative <- predict(
    model_conservative,
    x
)
pred_conservative[1:6]
y[1:6]
```


```{r}
model_conservative <- xgboost(
    x, y, nthreads = 1,
    nrounds = 5, # Rundy to w tym wypadku ilość modeli składowych
    max_depth = 3, # Maksymalna głębokość pojedynczego modelu
    reg_lambda = 0.7, # Regularyzacja L2 wag w liściach
    learning_rate = 0.1, # W jaki sposób zmniejsza się wartość kroku w algorytmie GB
    min_child_weight = 3
)
pred_conservative <- predict(
    model_conservative,
    x
)
pred_conservative[1:6]
y[1:6]
```
```{r}
model_conservative <- xgboost(
    x, y, nthreads = 1,
    nrounds = 5, # Rundy to w tym wypadku ilość modeli składowych
    max_depth = 2, # Maksymalna głębokość pojedynczego modelu
    reg_lambda = 0.5, # Regularyzacja L2 wag w liściach
    learning_rate = 0.15, # W jaki sposób zmniejsza się wartość kroku w algorytmie GB
    min_child_weight = 3
)
pred_conservative <- predict(
    model_conservative,
    x
)
pred_conservative[1:6]
y[1:6]
```
```{r}
model_conservative <- xgboost(
    x, y, nthreads = 1,
    nrounds = 5, # Rundy to w tym wypadku ilość modeli składowych
    max_depth = 2, # Maksymalna głębokość pojedynczego modelu
    learning_rate = 0.15, # W jaki sposób zmniejsza się wartość kroku w algorytmie GB
    min_child_weight = 3
)
pred_conservative <- predict(
    model_conservative,
    x
)
pred_conservative[1:6]
y[1:6]
```
Pełna lista parametrów:
<https://xgboost.readthedocs.io/en/release_3.0.0/parameter.html>

**1. Przetestuj kilka zestawów parametrów w oparciu o dokumentację. Kod
i wnioski zawrzyj poniżej.**

Drzewa możemy ograniczać na różny sposób, jednak to co rzuca mi się w oczy to ograniczenia na parametry drzewa - można oraniczać jaki rozmiar może przybrać oraz jakie dziecko musi zawierać minimalną ilość elementów aby doszło do podziału.

W ten sposób można unikać przeuczenia modelu, wydaje się że równie skutecznie co regularyzacja sama w sobie.

Możemy analizować jakość modelu w trakcie treningu

```{r xgboost-examine}
xgboost(
    x, y, nthreads = 1,
    eval_set = 0.2,
    monitor_training = TRUE,
    verbosity = 1,
    eval_metric = c("auc", "logloss"),
    nrounds = 5,
    max_depth = 2,
    reg_lambda = 0.5,
    learning_rate = 0.15
)
```

Oraz dokładnie przeanalizować parametry modelu

```{r xgboost model}
attributes(model) # Ogólne informacje na temat modelu w API R
xgb.importance(model) # Ważność cech
xgb.model.dt.tree(model) # Dokładny opis kolejnych drzew
```

**2. Który z predyktorów okazał się istotniejszy? Co o tym świadczy?**
Ostotniejszym okazała się długość niż dawka, zdecydowanie. Występowała częściej oraz wnosiła więcej informacji

Dla prezentacji CatBoost pobierzemy zbiór danych zarobkowych z UCI. Jest
domyślnie podzielony na treningowy i testowy, wiec dla własnych
zastosowań ponownie go połączymy.

```{python dataset}
url_train = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
url_test = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"

columns = [
    "age", "workclass", "fnlwgt", "education", "education-num",
    "marital-status", "occupation", "relationship", "race", "sex",
    "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"
]

df_train = pd.read_csv(url_train, header=None, names=columns, na_values=" ?", skipinitialspace=True)
df_test = pd.read_csv(url_test, header=0, names=columns, na_values=" ?", skipinitialspace=True, comment='|')

income_df = pd.concat([df_train, df_test], ignore_index=True)
income_df.info()
```

Zbiór zawiera dane kategoryczne, więc przed wykorzystaniem XGBoost
musielibyśmy przeprowadzić preprocessing. CatBoost natomiast domyślnie
wspiera tego typu dane.

```{python catboost prepare}
X = income_df.drop('income', axis=1)
y = income_df['income'].astype(str).str.rstrip('.')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

cat_features = np.where(X_train.dtypes == 'object')[0]
```

CatBoost wprowadza bardzo wygodną klasę pomocniczą Pool, która
reprezentuje zbiór danych.

```{python catboost pool}
train_pool = Pool(X_train, y_train, cat_features=cat_features)
test_pool = Pool(X_test, y_test, cat_features=cat_features)
```

Następnie przeprowadzamy klasyfikację. Catboost informuje nas o
postępach uczenia w jego trakcie.

```{python catboost model}
model = CatBoostClassifier(
    iterations=100,
    depth=4,
    learning_rate=0.1,
    loss_function='MultiClass',
    verbose=10
)

model.fit(train_pool)

preds_class = model.predict(test_pool)
preds_proba = model.predict_proba(test_pool)

print("Sample predictions:", preds_class[:10])
print("Sample probabilities:", preds_proba[:10])
```

Na koniec sprawdzamy ważność cech i macierz pomyłek

```{python catboost features}
feature_importance = model.get_feature_importance()
feature_names = X_train.columns

plt.barh(feature_names, feature_importance)
plt.title("CatBoost Feature Importance")
plt.tight_layout()
plt.show()
```
## Wartość 
```{python catboost conf matrix}
from catboost.utils import get_confusion_matrix

# Grupy to <=50k oraz >50k
get_confusion_matrix(model, test_pool)
```

**3. Wykorzystaj model boostingowy do predykcji jakości wina na
podstawie zbioru UCI winequality
<https://archive.ics.uci.edu/dataset/186/wine+quality>. Wybierz i
porównaj dwa rodzaje modeli (regersji, klasyfikacji, CatBoostRanker)**

```{r}
winequality_white <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep = ";")
winequality_red <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", sep = ";")
head(winequality_white)

winequality_white$type <- c('white')
winequality_red$type <- c('red')

winequality <- rbind(winequality_white, winequality_red)
winequality
```


```{python}
wine_df = pd.DataFrame(r.winequality)
import numpy as np

X = wine_df.drop('quality', axis=1)
y = wine_df['quality']

train_indecies, test_indecies = train_test_split(np.arange(X.shape[0]), test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = X.iloc[train_indecies], X.iloc[test_indecies], y.iloc[train_indecies], y.iloc[test_indecies]
wine_features = np.where(X_train.dtypes == 'object')[0]
```

```{r}
summary(winequality)


```

```{r}
X_train <- winequality[py$train_indecies, -c(12, 13)]
X_test <- winequality[py$test_indecies, -c(12, 13)]
y_train <- winequality[py$train_indecies, c(12)]
y_test <- winequality[py$test_indecies, c(12)]

model_wine <- xgboost(
    X_train, y_train, nthreads = 1,
    nrounds = 5, #
    max_depth = 4, 
    reg_lambda = 0.3, 
    learning_rate = 0.15, 
    min_child_weight = 3
)
y_pred <- predict(
    model_wine,
    X_test
)


# użyłem rmse, z racji iż drzewo nie przewiduje kategoriami, tylko liczbę rzeczywistą
library(Metrics)
rmse(y_pred, y_test)
```
```{r}
attributes(model_wine) 
xgb.importance(model_wine) 
xgb.model.dt.tree(model_wine) 
```



```{python catboost pool}
train_pool_wine = Pool(X_train, y_train, cat_features=wine_features)
test_pool_wine = Pool(X_test, y_test, cat_features=wine_features)
```

```{python catboost model}
model_wine = CatBoostClassifier(
    iterations=100,
    depth=4,
    learning_rate=0.1,
    loss_function='MultiClass',
    verbose=10
)

model_wine.fit(train_pool_wine)

preds_class = model_wine.predict(test_pool_wine)
preds_proba = model_wine.predict_proba(test_pool_wine)

print("Sample predictions:", preds_class[:10])
print("Sample probabilities:", preds_proba[:10])
```
```{python catboost features}
feature_importance = model_wine.get_feature_importance()
feature_names = X_train.columns

plt.clf()
plt.barh(feature_names, feature_importance)
plt.title("CatBoost Feature Importance")
plt.tight_layout()
plt.show()
model_wine.get_feature_importance()
```


## Tabela porównawcza

Lepiej widoczna w trybie wizualnym

| Cecha / Biblioteka           | **XGBoost**                                            | **LightGBM**                                         | **CatBoost**                                                                        |
|------------------|------------------|------------------|-------------------|
| **Model bazowy**             | Drzewa CART (głównie z binarnym rozgałęzieniem)        | Drzewa z podziałem liściowym (leaf-wise)             | Drzewa symetryczne (obustronnie zrównoważone)                                       |
| **Wydajność obliczeniowa**   | Wysoka, ale wolniejsza niż LightGBM                    | Bardzo wysoka – szybkie uczenie i predykcja          | Wysoka, szczególnie przy danych kategorycznych                                      |
| **Obsługa danych**           | Wymaga kodowania danych kategorycznych                 | Wymaga kodowania danych kategorycznych               | Wbudowana obsługa danych kategorycznych                                             |
| **Odporność na overfitting** | Dobra, możliwość regulacji poprzez regularizację L1/L2 | Może łatwo przeuczać przy złych parametrach          | Wysoka – zastosowano m.in. techniki porządkowania (ordered boosting)                |
| **Skalowalność**             | Dobrze skalowalny, wspiera przetwarzanie równoległe    | Bardzo dobrze skalowalny (dla dużych zbiorów)        | Skalowalny, choć gorzej niż pozostałe                                               |
| **Zalety**                   | Stabilność, dojrzała dokumentacja, elastyczność        | Bardzo szybkie działanie, niskie zużycie pamięci     | Brak konieczności kodowania zmiennych kategorycznych, dobre wyniki „out-of-the-box” |
| **Wady**                     | Konieczność ręcznego przygotowania danych              | Wrażliwość na przeuczenie, trudniejsza interpretacja | Dłuższy czas treningu dla danych liczbowych                                         |
