---
title: "MARS i AutoML"
output: html_document 
editor_options: 
  markdown: 
    wrap: 72
---

## Wprowadzenie

Dzisiejsze laboratorium podzielone jest na dwie części przedstawiające
dwa nowe narzędzia:

-   MARS (Multivariate Adaptive Regression Splines)
-   AutoML

## MARS

Jest to adaptacyjna metoda regresji dobrze dostosowana do problemów
wielowymiarowych (z dużą liczbą predyktorów).

MARS rozpoczyna od wartości stałej, by sekwencyjnie dobierać najlepsze
punkty przełamania dla dostępnych zmiennych tak, by zminimalizować
rezydualną sumę kwadratów (RSS). Działaniem tym przypomina drzewo
regresji, ale jego funkcjami bazowymi nie są stałe a spline'y. Spline'y
mają tą przydatną właściwość, że mogą być funkcjami wiel zmiennych, co
pozwala nam wyłapywać zależności między predyktorami.

Na tym podobieństwa do drzew się nie kończą. MARS wykonuje również
pruning, usuwając na koniec najmniej wnoszące spline'y. Dodatkową
funkcjonalnością tego modelu jest wynikająca z niego selekcja cech.
Jeśli zmienna nie przyczynia się do redukcji błędu dopasowania, nie
zostanie użyta do stworzenia funkcji bazowych.

```{r setup}
library(earth)
library(MASS)

set.seed(123)

data(Boston)
str(Boston)
summary(Boston)
```

Dopasujmy podstawowy model MARS. Jak widać poniżej, wykorzystuje taki
sam interfejs formuły, co regresja liniowa.

```{r basic_mars}
mars_model <- earth(medv ~ ., degree = 3, data = Boston)

# Oraz standardowo - summary
summary(mars_model)
```

**1. Zwiększ wartość parametru degree. Co się zmieniło i dlaczego? **
Po zwiększeniu 'degree' zwiększona została ilość zmiennych w jednym punkcie przełamania - co oznacza, że teraz Spline są do 3 wymiarów na raz, a nie tylko po jednej zmiennej

Model możemy wypisać i zwizualizować

```{r model_structure}
print(mars_model)
plot(mars_model)
```

Funkcja evimp zwraca istotność zmiennych

```{r variable_importance}
evimp(mars_model)
plot(evimp(mars_model))
```

Modele MARS przyjmują zestaw parametrów związany zarówno z modelem
bazowym (splinem), jak i całą metodą.

```{r tuning}
mars_tuned <- earth(medv ~ ., data = Boston,
                   nk = 25,        # Maksymalna liczba funkcji bazowych w modelu przed pruningiem
                   degree = 2,     # Maksymalny stopień interakcji między zmiennymi
                   thresh = 0.001, # Próg przycinania funkcji bazowych
                   nprune = 12)    # Ile funkcji bazowych wyciąć w pruningu

# Porównajmy modele (GCV to Generalized Cross Validation - w praktyce zastępuje adjusted R-squared dla MARS)
cat("Basic model GCV:", mars_model$grsq, "\n")
cat("Basic model R-squared:", mars_model$rsq, "\n")
cat("Tuned model GCV:", mars_tuned$grsq, "\n")
cat("Tuned model R-squared:", mars_tuned$rsq, "\n")
```

**2. Wykonaj eksperyment dobierając różne wartości parametrów na
podstawie dokumentacji. Które są Twoim zdaniem najistotniejsze? Za co
odpowiadają?**

Wydaje się że najważniejszymi parametrami będą:
- nk oraz thresh ze względu na limit drzewa
- degree - bo limituje ile można naraz zmiennych brać pod uwagę w węźle

Wbrew moim przewidywaniom `penalty` szczególnie przy większej wymiarowości traciła na znaczeniu

```{r tuning}
mars_tuned <- earth(medv ~ ., data = Boston,
                   nk = 25,        # Maksymalna liczba funkcji bazowych w modelu przed pruningiem
                   degree = 5,     # Maksymalny stopień interakcji między zmiennymi
                   thresh = 0.001, # Próg przycinania funkcji bazowych
                   nprune = 12)    # Ile funkcji bazowych wyciąć w pruningu

# Porównajmy modele (GCV to Generalized Cross Validation - w praktyce zastępuje adjusted R-squared dla MARS)
cat("Basic model GCV:", mars_model$grsq, "\n")
cat("Basic model R-squared:", mars_model$rsq, "\n")
cat("Tuned model GCV:", mars_tuned$grsq, "\n")
cat("Tuned model R-squared:", mars_tuned$rsq, "\n")
```
```{r tuning}
mars_tuned <- earth(medv ~ ., data = Boston,
                   nk = 25,        # Maksymalna liczba funkcji bazowych w modelu przed pruningiem
                   degree = 5,     # Maksymalny stopień interakcji między zmiennymi
                   penalty = -1,
                   thresh = 0.001, # Próg przycinania funkcji bazowych
                   nprune = 12)    # Ile funkcji bazowych wyciąć w pruningu

# Porównajmy modele (GCV to Generalized Cross Validation - w praktyce zastępuje adjusted R-squared dla MARS)
cat("Basic model GCV:", mars_model$grsq, "\n")
cat("Basic model R-squared:", mars_model$rsq, "\n")
cat("Tuned model GCV:", mars_tuned$grsq, "\n")
cat("Tuned model R-squared:", mars_tuned$rsq, "\n")
```
Pełny trening modelu z wykorzystaniem walidacji krzyżowej:

```{r cross_validation}
n <- nrow(Boston)
train_idx <- sample(1:n, size = 0.7 * n)
train_data <- Boston[train_idx, ]
test_data <- Boston[-train_idx, ]

mars_train <- earth(medv ~ ., data = train_data, nfold = 5)

pred_test <- predict(mars_train, test_data)
pred_train <- predict(mars_train, train_data)

rmse_train <- sqrt(mean((train_data$medv - pred_train)^2))
rmse_test <- sqrt(mean((test_data$medv - pred_test)^2))

cat("Training RMSE:", round(rmse_train, 3), "\n")
cat("Test RMSE:", round(rmse_test, 3), "\n")
```

Oraz wizualizacja modelu względem pełnego zbioru danych:

```{r predictions}
predictions <- predict(mars_model, Boston)

plot(Boston$medv, predictions, 
     xlab = "Actual Median Home Value", 
     ylab = "Predicted Median Home Value",
     main = "MARS: Actual vs Predicted Values")
abline(0, 1, col = "red", lty = 2)

rsq <- cor(Boston$medv, predictions)^2
text(40, 15, paste("R² =", round(rsq, 3)), cex = 1.2)

residuals <- Boston$medv - predictions
plot(predictions, residuals,
     xlab = "Predicted Values", ylab = "Residuals",
     main = "Residual Plot")
abline(h = 0, col = "red", lty = 2)
```

Na koniec porównajmy MARS z regresją liniową:

```{r comparison}
lm_model <- lm(medv ~ ., data = Boston)

lm_pred <- predict(lm_model, Boston)
mars_pred <- predict(mars_model, Boston)

lm_rmse <- sqrt(mean((Boston$medv - lm_pred)^2))
mars_rmse <- sqrt(mean((Boston$medv - mars_pred)^2))

cat("=== Regresja liniowa ===\n")
cat("R-squared:", summary(lm_model)$r.squared, "\n")
cat("Adjusted R-squared:", summary(lm_model)$adj.r.squared, "\n")
cat("RMSE:", round(lm_rmse, 3), "\n")

cat("\n=== MARS ===\n")
cat("R-squared:", mars_model$rsq, "\n")
cat("GCV R-squared:", mars_model$grsq, "\n")
cat("RMSE:", round(mars_rmse, 3), "\n")
```

## AutoML

AutoML, to rozbudowane narzędzia, których zadaniem jest jak najbardziej
zautomatyzować proces uczenia maszynowego. Za ich pomocą można
relatywnie niskim kosztem pozyskać często bardzo dobry model bazowy.
Dostarczają też wiele informacji odnośnie dopasowanych modeli, które
można wykorzystać w kolejnych etapach uczenia.

Narzędzia AutoML wykonają następujące czynności: - podstawowa obróbka
danych (wartości brakujące, zmienne kategoryczne, skalowanie) -
wytrenują wiele modeli z różnymi parametrami w poszukiwaniu
najlepszego - dostarczą podstawowej interpretacji modeli

Czego AutoML nie zrobią? - nie zdefiniuje konkretnego problemu na
podstawie wymagań klienta - nie zaprojektuje nowych zmiennych - nie
przetworzy niestandardowych danych - nie wykona wszystkich zadań
towarzyszących uczeniu maszynowemu (infrastruktura, debugging, etc)

Przykłady narzedzi AutoMl: - H2O.ai - PyCaret - FLAML (Microsoft) -
AutoGluon (Amazon)

Przykład AutoMl w oparciu o wine quality dataset:

```{python load_wine}
import pandas as pd

# Zbiór danych Wine quality z UCI
url_white = "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
url_red = "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"

# Pandas pozwala ładować dane w oparciu o URL
wine_white = pd.read_csv(url_white, sep=';')
wine_red = pd.read_csv(url_red, sep=';')
```

```{python combine_wine}
wine_white['type'] = 'white'
wine_red['type'] = 'red'

wine_df = pd.concat([wine_white, wine_red], ignore_index=True)

wine_df.info()
```

```{python split}
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(wine_df, test_size=0.7)
```

Od tego momentu zaczynamy pracować z FLAML

Trening:

```{python flaml_train}
from flaml import AutoML

X_train = train_df.drop(columns=['quality'])
y_train = train_df['quality']

settings = {
    "time_budget": 60,  # jak długo ma się uczyć w sekundach
    "metric": "r2",  # metryka oceny jakości
    "task": "regression",  # typ zadania
    "log_file_name": "wine_experiment.log",  # plik z logami
    "seed": 7654321,  # ziarno losowe
}

automl = AutoML()
automl.fit(X_train, y_train, **settings)
```

Predykcja:

```{python flaml_predict}
from sklearn.metrics import r2_score

X_test = test_df.drop(columns=['quality'])
y_test = test_df['quality']

y_pred = automl.predict(X_test)
r2_score(y_test, y_pred)
```

Interpretacja:

```{python flaml_interpret}
print(automl.model.estimator)
print("Best hyperparmeter config:", automl.best_config)
print("Best r2 on validation data: {0:.4g}".format(1 - automl.best_loss))
print("Training duration of best run: {0:.4g} s".format(automl.best_config_train_time))
```

```{python flaml_features}
import matplotlib.pyplot as plt

plt.close('all')
plt.barh(automl.feature_names_in_, automl.feature_importances_)
plt.show()
```

```{python flaml_learning}
from flaml.automl.data import get_output_from_log
import numpy as np

plt.close('all')

time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = get_output_from_log(filename=settings['log_file_name'], time_budget=60)
plt.title('Learning Curve')
plt.xlabel('Wall Clock Time (s)')
plt.ylabel('Validation r2')
plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')
plt.show()
```

**3. Pobierz dane z konkursu i wyślij swoje pierwsze wyniki w oparciu o
AutoML**
```{python load data}
receptor_train = pd.read_csv('train.csv')
receptor_test = pd.read_csv('test.csv') 

X_receptors = receptor_train.drop(columns=['activity'])
y_receptors = receptor_train['activity']
run_iter = 0
```

```{python train model}
settings = {
    "time_budget": 120,  # jak długo ma się uczyć w sekundach
    "task": "binary",  # typ zadania
    'metric': 'f1',
#    'average' : 'weighted',
    "log_file_name": f"receptors_train_{run_iter}.log",  # plik z logami
    "seed": 123456,  # ziarno losowe
}

automl = AutoML()
automl.fit(X_receptors, y_receptors, **settings)
run_iter += 1
```

```{python evaluation}
y_pred = automl.predict(receptor_test)

result = pd.DataFrame({'ID' : receptor_test['ID'], 'activity' : y_pred})

result.to_csv('result0.csv', index=False)
```

```{python}
len(receptor_test)
```